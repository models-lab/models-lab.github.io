---
layout: post
title:  ModelSet - A dataset of software models for ML
date: 2021-11-19 15:00:00
description: ModelSet is a dataset of software models intended to foster the application of ML techniques to improve modelling. This article describes ModelSet and how to use for building a classifier which infers the category of a given meta-model.
---

An important shortcoming of current approaches for applying Machine
Learning (ML) to address problems related to Model-Driven Engineering
is the lack of curated datasets of software models. We believe that
there are several reasons for this, including, including the lack of
large collections of good quality models, the difficulty to label
models due to the required domain expertise, and the relative
immaturity of the application of ML to MDE.

To address this problem (at least partially), we created 
**ModelSet**, which is a dataset of software models intented to help in the application of machine learning techniques to solve modelling tasks. Its main features are the following:

* It contains more than 5.000 Ecore models (extracted from [GitHub](https://github.com/)) and more than 5.000 UML models (extracted from [GenMyModel](https://www.genmymodel.com/)).

* The models has been labelled with its category, which represents a type of models sharing a similar application domain. The following chart contains a summary of the main categories (yes! people like building `state machine` meta-models and UML models describing the `shopping` domain!).

<p align="center">
  <img src="/assets/posts/modelset/category.svg" alt="Main categories in ModelSet"/>
</p>

* In addition, ModelSet contains additional labels which provides more semantic information. For instance, a model can be labelled with `category: statemachine`, an additional label named tag with value `timed` to indicate its variant, and label purpose `teaching` to indicate that this particular model is being used for teaching purposes, In total, there are more than 28,000 labels.

<p align="center">
  <img src="/assets/posts/modelset/cloud.png" width="300px" alt="Tag cloud"/>
</p>

<!-- We could add an example model with its annotations -->


We foresee some **potential applications** ModelSet, like the following:

* Classification approaches in which the target variables are the labels. 
* Recommender systems (e.g., suggesting attribute names/types, etc.)
* Evaluation of clustering methods, in which the labels provide the ground truth about the clusters.
* Spurious model identification (i.e., using “dummy” labels)
* Evaluation of ML models via train-test-eval splitting in a stratified fashion using the labels.
* Train embeddings based on labels (i.e., for clone detection using the resulting vector). 
* Empirical analysis of the modelling domain.

Anyway, we are sure that other researchers/modellers interested in ML will find other interesting areas of application. 

If you need a larger collection of models for unsupervised approaches, you can also checkout the models available at [MAR](http://mar-search.org) (click on the Status tab). There are more than 500,000 models of different types, including Ecore, UML, Archimate, BPMN, etc. 



The details of ModelSet has been presented in this paper (which is freely available):

* [José Antonio Hernández López, Javier Luis Cánovas Izquierdo, and Jesús Sánchez Cuadrado. "ModelSet: a dataset for machine learning in model-driven engineering." Software and Systems Modeling (2021)](https://link.springer.com/article/10.1007/s10270-021-00929-3)

In this post we will explain some practical details about ModelSet: how to install it, its structure, how to load the dataset and we will show a concrete usage example to address  classification task: inferring the category of a model.

# Installation

First of all, you need to download and install ModelSet. 

 1. Download the package containing the raw models and the associated databases. Available at [https://github.com/modelset/modelset-dataset/releases](https://github.com/modelset/modelset-dataset/releases).
 2. Unzip the package in some local folder
 3. Install the Python library using pip. This will allow us to easily use ModelSet with standard ML libraries.
    * `pip install modelset-py`
    * If you have downloaded the source code of the library from [GitHub repository](http://github.com/modelset/modelset-py) ,
      then use `sys.path.append("/path/to/modelset-py/src")` as a shortcut to load it dynamically.
    

# Structure of the dataset

The dataset basically consists of: a) the raw models, b) the databases with the labels and information about the models and c) alternative serializations of the models (e.g., as text files). Here is the structure that you will find when you unzip the package.

```
[+] datasets
    [+] dataset.ecore
        [+] data 
	        [+] ecore.db
		        - The database with the labels for the Ecore models
	        [+] analysis.db 
		        - Statistics about the Ecore models
    [+] dataset.genmymodel
	        [+] genmymodel.db
		        - The database with the labels for the UML models
	        [+] analysis.db
		        - Statistics about the UML models
[+] raw-data
    [+] repo-ecore-all
        - The .ecore models that has been labelled
    [+] repo-genmymodel-uml
        - The UML models that has been labelled, stored as .xmi files 
[+] txt
    - A mirror of raw-data but with 1-gram encoding of the models,
	  that is for each model a textual file with the strings of the model.
```

The databases are just SQLite databases that you can manipulate using any SQLite connector (e.g., in Java or Python). 
The `ecore.db` and `genmymodel.db` files contain the databases with the labels associated to the models. You can open it with the `sqlite` command and the main part of the schema is simply:

```sql
sqlite> . schema
CREATE TABLE models (
    id varchar(255) PRIMARY KEY,
    repo varchar(255) NOT NULL,
    filename text NOT NULL
);
CREATE TABLE metadata (
    id varchar(255) PRIMARY KEY,
    metadata text NOT NULL,
    json text
);
```

The labels are stored in the `metadata` table, specifically the `metadata` column contains the labels as entered by the user who performed the labelling).
To facilitate its processing the `json` column contains a JSON representation of the labels. For instance, the following shows the labels in a model `FaultTree.ecore` serialized in JSON.

```json
{ 
  "category": ["fault-tree"],
  "tags": ["safety", "hazard"],
  "tool": ["osate2"]
}
```

It is possible to directly interact with the SQLite database to perform exploratory queries. For instance, the following query shows the number of models per category in the Ecore database. It uses the `json_extract` function to query the associated JSON that contains the metadata. 

```sql
$ sqlite datasets/dataset.ecore/ecore.db
sqlite> select json_extract(md.json, '$.category[0]') as category, count(*) as total
  from models m join metadata md on m.id = md.id
  group by category
  order by total desc;

category     total
--------     -----
dummy         729
statemachine  392
petrinet      236
library       235
modelling     209
class-diagram 182
gpl           180
```


The `analysis.db` database has the same schema as the databases provided by [MAR](http://mar-search.org). It contains statistics about the model. In the case of the Ecore dataset, it also contains information about design smells found in each model.

```sql
$ sqlite3 repo-github-ecore/analysis.db
sqlite> . schema
CREATE TABLE models (
    id            varchar(255) PRIMARY KEY,
    relative_file text NOT NULL,
    hash          text NOT NULL,
    status        varchar(255) NOT NULL,
    metadata_document TEXT,
    duplicate_of  varchar(255)
);
CREATE TABLE stats (
    id    varchar(255) NOT NULL,
    type  varchar (255) NOT NULL,
    count integer NOT NULL
);
```

The following query shows statistics about the models in the Ecore dataset. `elements` refers to the total number of elements, and the other types refers to meta-elements (e.g., EAttribute, EClass, etc.).

```sql
sqlite> select type, avg(count) from stats group by type;
type        avg(count) 
----------  ----------
attributes     16.31
classes        25.74
datatypes       1.23
elements      206.32
enum            1.24
packages        1.46
references     26.82
```


# Example

In the rest of the post we will develop a concrete example, using ModelSet
to build a classifier able to infer the category of a given Ecore model.
We will use the [ModelSet Python library](http://github.com/modelset/modelset-py) to access the dataset and [pandas](https://pandas.pydata.org/) and [scikit-learn](https://scikit-learn.org) to manipulate the dataset and train the model.

The implementation of this example is available in our repository of
ModelSet examples. You can download it [here](https://github.com/modelset/modelset-apps/blob/master/python/modelset-tutorial-categories.ipynb).



## Loading the dataset

The ModelSet library offers a convenient interface to dump the contents of the underlying database into a dataframe. In particular, there are several features available in the output dataframe:

 * The identifier of the model
 * The category of the model (manually labelled). Reflects the domain of the model.
 * Associated tags (zero or more manually labelled) which provide additional insights about the type of model.
 * The language of the model (typically english)
 * Basic stats. In the case of Ecore, number of elements, references, classes, attributes, packages, enumerations and datatypes



```python
import sys
import pandas as pd
import os

import modelset.dataset as ds
```


```python
dataset = ds.load(MODELSET_HOME, modeltype = 'ecore', selected_analysis = ['stats'])
# You can just use: ds.load(MODELSET_HOME, modeltype = 'ecore') to speedup the loading if you don't need the stats
```

Convert the dataset into a Pandas dataframe. There are two methods: 
 * `to_df()` converts the complete dataset. 
 * `to_normalized_df()` only considers examples with a minimum number of examples (7 by default), written in english and removing special categories (dummy and unknown).
 
```python
modelset_df = dataset.to_normalized_df()
# You can configure the elements of the dataframe:
# modelset_df = dataset.to_normalized_df(min_ocurrences_per_category = 7, languages = ['english'], remove_categories = ['dummy', 'unknown'])
```


```python
modelset_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>category</th>
      <th>tags</th>
      <th>language</th>
      <th>references</th>
      <th>elements</th>
      <th>classes</th>
      <th>attributes</th>
      <th>packages</th>
      <th>enum</th>
      <th>datatypes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>repo-ecore-all/data/AmerPecuj/MBSE/dk.dtu.comp...</td>
      <td>petrinet</td>
      <td>behaviour</td>
      <td>english</td>
      <td>7</td>
      <td>27</td>
      <td>6</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>repo-ecore-all/data/nlohmann/service-technolog...</td>
      <td>petrinet</td>
      <td>behaviour</td>
      <td>english</td>
      <td>13</td>
      <td>92</td>
      <td>15</td>
      <td>16</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>repo-ecore-all/data/damenac/puzzle/examples/em...</td>
      <td>education</td>
      <td>domainmodel</td>
      <td>english</td>
      <td>4</td>
      <td>37</td>
      <td>4</td>
      <td>12</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>repo-ecore-all/data/francoispfister/diagraph/o...</td>
      <td>statemachine</td>
      <td>behaviour</td>
      <td>english</td>
      <td>7</td>
      <td>87</td>
      <td>9</td>
      <td>13</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>repo-ecore-all/data/gssi/metamodelsdataset-ECM...</td>
      <td>petrinet</td>
      <td>behaviour</td>
      <td>english</td>
      <td>3</td>
      <td>17</td>
      <td>4</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>5468</th>
      <td>repo-ecore-all/data/Barros-Lucas/DSL_State_Int...</td>
      <td>statemachine</td>
      <td>behaviour</td>
      <td>english</td>
      <td>3</td>
      <td>22</td>
      <td>5</td>
      <td>4</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5469</th>
      <td>repo-ecore-all/data/luciuscode/test/projectStr...</td>
      <td>library</td>
      <td>domainmodel</td>
      <td>english</td>
      <td>4</td>
      <td>34</td>
      <td>6</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5470</th>
      <td>repo-ecore-all/data/BlackBeltTechnology/emfbui...</td>
      <td>company</td>
      <td>NaN</td>
      <td>english</td>
      <td>2</td>
      <td>22</td>
      <td>5</td>
      <td>4</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5473</th>
      <td>repo-ecore-all/data/mathiasnh/TDT4250-Assignme...</td>
      <td>education</td>
      <td>university|domainmodel</td>
      <td>english</td>
      <td>24</td>
      <td>101</td>
      <td>11</td>
      <td>12</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5474</th>
      <td>repo-ecore-all/data/agacek/jkind-xtext/jkind.x...</td>
      <td>simple-pl</td>
      <td>expressions|types|lustre|programming</td>
      <td>english</td>
      <td>55</td>
      <td>214</td>
      <td>44</td>
      <td>14</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>4230 rows × 11 columns</p>
</div>



## Spliting the dataset

To train our model we are interested on the category attribute, which will be our target variable (the label that we want to predict) and we are going to use the model identifiers as input data because we will use them to lookup the corresponding textual representation (see below). 

We need to split our dataset into training and test, so that we can evaluate later the accuracy of our model.


```python
from sklearn.model_selection import train_test_split

# These dataframes are vectors
ids     = modelset_df['id']
labels  = modelset_df['category']

train_X, test_X, train_y, test_y = train_test_split(ids, labels, test_size=0.2, random_state=42)
```

## Selecting features

A neural networks takes an input a numerical vector. So, we need a way to encode a model into a vector. A simple way is to use a TF-IDF encoding. Essentially, [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a measure of the relevance of a word by comparing the number of times that a word appears in a document with respect to the number of documents in which the word appears. 

To apply TF-IDF, the first thing that we need to do is to extract a textual representation of each model. We use the `txt_file` method to obtain the path to the text file associated with a given model. This is a feature provided by ModelSet: for each model there is already `.txt` which contains its 1-gram (i.e., the values of the string attributes).  

Then, we can easily compute the TF-IDF using scikit-learn. The `X` and `T` matrices contain one row per model with a number of columns equals to the number of words in the models.  


```python
import numpy as np 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer

train_filenames = [ dataset.txt_file(id) for id in train_X ]
test_filenames  = [ dataset.txt_file(id) for id in test_X ]

vectorizer = TfidfVectorizer(input='filename', min_df = 2)
X = vectorizer.fit_transform(train_filenames)
T = vectorizer.transform(test_filenames)
```


```python
# The output of the TF-IDF vectorization is a large matrix with len(train_X) rows and 
# as many columns as words in the vocabulary
X.shape
```

    (3384, 24810)



## Training

We use a neural network with one hidden layer as our model. This is straightforward with scikit-learn.


```python
from sklearn.neural_network import MLPClassifier

#input_layer = X.shape[1]
clf = MLPClassifier(solver='adam', learning_rate_init=0.01, hidden_layer_sizes=(64), random_state=1)
clf.fit(X, train_y)
```

## Evaluation


```python
from sklearn.metrics import classification_report,confusion_matrix
```

First, we evaluate the results obtained in the training set. In particular, we focus on the accuracy (the fraction of correctly classified examples).


```python
predict_train = clf.predict(X)
# print(confusion_matrix(train_y, predict_train))
train_report = classification_report(train_y, predict_train, output_dict = True)
print("Training accuracy: ", train_report['accuracy'])
```

```
Training accuracy:  0.9994089834515366
```

Then, we evaluate the classifier over the test set. As can be seen the results are good, and in principle, we can assume that our model is ok and we can use it in practice.


```python
predict_test = clf.predict(T)
test_report = classification_report(test_y, predict_test, output_dict = True)
print("Test accuracy: ", test_report['accuracy'])
```

```
Test accuracy:  0.9030732860520094
```

# Practical usage

We have used ModelSet to enhance the [MAR search engine](http://mar-search.org). In particular, we use the model described above to
infer the the category of the models shown as search results.
In the image below the dropdown menu allows the user to filter the search results (label 1) and the little badges (label 2) are the categories and tags inferred per each model.

There are more than 17,000 Ecore models in MAR, so it is not feasiable to label all of them by hand. We have used the a ML model as the one trained in the example to infer the category of each model and so generate the associated badge.


<p align="center">
  <img src="/assets/posts/modelset/mar.png" width="800px" alt="MAR"/>
</p>

# Summary

This post is an introduction to ModelSet. We hope that this is a useful resource for the community.
